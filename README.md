# LLM-Based Autocomplete

A simple, lightweight autocomplete system powered by transformer-based language models.

## Features
- **ðŸ¤– LLM-powered predictions** using DistilGPT-2
- **âš¡ Fast inference** - typically 0.1-0.3s per prediction
- **ðŸš€ Simple API** - easy to integrate
- **ðŸ’» CPU-friendly** - no GPU required (but supports GPU if available)
- **ðŸ“¦ Lightweight** - uses efficient DistilGPT-2 model (82M parameters)

## Installation

```bash
pip install -r requirements.txt
```

**Dependencies:**
- torch
- transformers
- tokenizers
- accelerate

## Quick Start

```python
from llm_autocomplete import LLMAutocompleteModel

# Initialize model
model = LLMAutocompleteModel()

# Get predictions
predictions = model.predict_next_words("I want", num_predictions=3)

# Display results
for word, confidence in predictions:
    print(f"'{word}' (confidence: {confidence:.3f})")
```

## Usage

### Python API

```python
from llm_autocomplete import LLMAutocompleteModel

# Initialize the model
model = LLMAutocompleteModel()

# Get predictions for a word or phrase
predictions = model.predict_next_words("hello", num_predictions=5)

# predictions returns: [(word, confidence), ...]
for word, conf in predictions:
    print(f"{word}: {conf:.2f}")
```

### Command Line

Run the interactive demo:
```bash
python3 llm_autocomplete.py
```

### Run Tests

```bash
python3 test_llm_autocomplete.py
```

## API Reference

### LLMAutocompleteModel

#### `__init__(model_name="distilgpt2")`

Initialize the autocomplete model.

**Parameters:**
- `model_name` (str): Hugging Face model name. Default: `"distilgpt2"`

**Example:**
```python
# Use default model
model = LLMAutocompleteModel()

# Or specify a different model
model = LLMAutocompleteModel(model_name="gpt2")
```

#### `predict_next_words(word, num_predictions=3)`

Predict the next words given input text.

**Parameters:**
- `word` (str): Input text to predict from
- `num_predictions` (int): Number of predictions to return (default: 3)

**Returns:**
- `List[Tuple[str, float]]`: List of (predicted_word, confidence) tuples

**Example:**
```python
predictions = model.predict_next_words("The weather is", 5)
# Returns: [('nice', 0.95), ('good', 0.85), ('great', 0.75), ...]
```

#### `get_model_info()`

Get information about the loaded model.

**Returns:**
- `Dict`: Dictionary containing model information

**Example:**
```python
info = model.get_model_info()
print(f"Model: {info['model_name']}")
print(f"Device: {info['device']}")
```

## Examples

### Basic Word Completion
```python
model = LLMAutocompleteModel()

# Complete a partial sentence
predictions = model.predict_next_words("I want to", 3)
print(predictions)
# Output: [('see', 0.95), ('know', 0.85), ('go', 0.75)]
```

### Multiple Predictions
```python
# Get more prediction options
predictions = model.predict_next_words("The cat", 5)
for i, (word, conf) in enumerate(predictions, 1):
    print(f"{i}. '{word}' (confidence: {conf:.2f})")
```

### Using Different Models
```python
# Use larger GPT-2 model for better quality
model = LLMAutocompleteModel(model_name="gpt2")

# Or GPT-2 Medium (355M parameters)
model = LLMAutocompleteModel(model_name="gpt2-medium")
```

## Model Information

- **Default Model**: DistilGPT-2
- **Parameters**: 82M
- **Context Window**: 1024 tokens
- **Framework**: PyTorch + Hugging Face Transformers
- **Device Support**: CPU (default) / CUDA (auto-detected)

### Generation Parameters:
- `max_new_tokens`: 5
- `temperature`: 0.8
- `top_p`: 0.9
- `top_k`: 50

## File Structure

```
autocomplete/
â”œâ”€â”€ llm_autocomplete.py              # Main module
â”œâ”€â”€ test_llm_autocomplete.py         # Tests
â”œâ”€â”€ requirements.txt                 # Dependencies
â””â”€â”€ README.md                        # This file
```

## Performance

- **Response time**: 0.1-0.3s per prediction (CPU)
- **Memory usage**: ~500MB-1GB
- **First run**: Downloads model (~500MB) from Hugging Face
- **Subsequent runs**: Uses cached model

## Troubleshooting

### Installation Issues
```bash
# Upgrade dependencies
pip install --upgrade torch transformers tokenizers accelerate
```

### Slow Performance
- First run downloads the model from Hugging Face Hub
- Subsequent runs are faster using cached model
- For better performance, use a GPU if available
- Consider using smaller models for resource-constrained environments

### Import Errors
Ensure all dependencies are installed:
```bash
pip install -r requirements.txt
```

## Available Models

You can use different models from Hugging Face:

| Model | Parameters | Quality | Speed |
|-------|-----------|---------|-------|
| `distilgpt2` | 82M | Good | Fast |
| `gpt2` | 124M | Better | Medium |
| `gpt2-medium` | 355M | Great | Slower |
| `gpt2-large` | 774M | Best | Slow |

**Note:** Larger models provide better predictions but require more memory and time.

## Advanced Usage

### Custom Generation Parameters

Modify the generation parameters in `llm_autocomplete.py`:

```python
results = self.generator(
    input_text,
    max_new_tokens=10,      # Generate more tokens
    temperature=0.7,        # Lower = more conservative
    top_p=0.95,            # Nucleus sampling
    top_k=40               # Top-k sampling
)
```

### Batch Processing

```python
model = LLMAutocompleteModel()

phrases = ["I want", "The cat", "Hello"]
results = {}

for phrase in phrases:
    results[phrase] = model.predict_next_words(phrase, 3)

for phrase, predictions in results.items():
    print(f"{phrase}: {predictions}")
```

## License

MIT License

---

**Version**: 1.0  
**Last Updated**: October 2025  
**Status**: âœ… Production Ready
